{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c80999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import textwrap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#from typing import List, Dict, Any, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(torch.cuda.is_available())   # Should be True\n",
    "print(torch.cuda.device_count())   # Should be >= 1\n",
    "print(torch.cuda.get_device_name(0))  # Should print your GPU's name\n",
    "print(torch.version.cuda)   # Should show CUDA version\n",
    "print(torch.backends.cudnn.version())  # Should print a version number\n",
    "\n",
    "# Fix SSL certificate issues\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Hugging Face imports\n",
    "from transformers import (\n",
    "    AutoProcessor, \n",
    "    AutoModelForVisualQuestionAnswering,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    VisionEncoderDecoderModel,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModel\n",
    ")\n",
    "import open_clip\n",
    "from open_clip import create_model_from_pretrained, get_tokenizer\n",
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "import evaluate\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load METEOR metric\n",
    "meteor = evaluate.load('meteor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bf65f3",
   "metadata": {},
   "source": [
    "### Fine-Tune BLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2a559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "model = AutoModelForVisualQuestionAnswering.from_pretrained(\n",
    "    \"Salesforce/blip-vqa-base\", \n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752bc4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Path-VQA dataset from Hugging Face\n",
    "dataset = load_dataset(\"flaviagiammarino/path-vqa\")\n",
    "\n",
    "print(f\"Train: {len(dataset['train'])} samples\")\n",
    "print(f\"Validation: {len(dataset['validation'])} samples\")\n",
    "print(f\"Test: {len(dataset['test'])} samples\")\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0b4255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BlipDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]  # always int index!\n",
    "        img = item[\"image\"].convert('RGB')\n",
    "        encoding = self.processor(\n",
    "            images=img,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True\n",
    "        )\n",
    "        return {\n",
    "            \"pixel_values\": {k: v.squeeze() for k, v in encoding.items()}['pixel_values'],  # shape (1, 3, H, W)\n",
    "            \"question\": item[\"question\"],\n",
    "            \"answer\": item[\"answer\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942dd42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = random.sample(range(len(train_dataset)), 8000)\n",
    "val_indices = random.sample(range(len(val_dataset)), 800)\n",
    "\n",
    "train_list = [train_dataset[i] for i in train_indices]\n",
    "val_list   = [val_dataset[i] for i in val_indices]\n",
    "\n",
    "train_ds = BlipDataset(train_list, processor)\n",
    "val_ds   = BlipDataset(val_list, processor)\n",
    "\n",
    "max_q = max(len(item[\"question\"].split()) for item in train_list)\n",
    "max_a = max(len(item[\"answer\"].split()) for item in train_list)\n",
    "\n",
    "print(f\"Maximum character length of training question: {max_q}\")\n",
    "print(f\"Maximum character length of training answer: {max_a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dde75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, module in model.named_modules():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a423d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=None,\n",
    "    #inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\n",
    "        # vision encoder, fused QKV + output\n",
    "        \"self_attn.qkv\",\n",
    "        \"self_attn.projection\",\n",
    "\n",
    "        # text encoder self-attention\n",
    "        \"attention.self.query\",\n",
    "        \"attention.self.key\",\n",
    "        \"attention.self.value\",\n",
    "        \"attention.output.dense\",\n",
    "\n",
    "        # text decoder cross-attention\n",
    "        \"crossattention.self.query\",\n",
    "        \"crossattention.self.key\",\n",
    "        \"crossattention.self.value\",\n",
    "        \"crossattention.output.dense\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.to(device)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf2821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # process image \n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "\n",
    "    # process question\n",
    "    question_inputs = processor.tokenizer(\n",
    "        [item[\"question\"] for item in batch],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=max(max_q, max_a)\n",
    "    )\n",
    "\n",
    "    # process answer\n",
    "    answer_inputs = processor.tokenizer(\n",
    "        [item[\"answer\"] for item in batch],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=max(max_q, max_a)\n",
    "    )\n",
    "\n",
    "    output = {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": question_inputs[\"input_ids\"],\n",
    "        \"attention_mask\": question_inputs[\"attention_mask\"],\n",
    "        \"labels\": answer_inputs[\"input_ids\"]#.clone()\n",
    " \n",
    "    }\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94432b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\".\\model\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=50,\n",
    "    dataloader_num_workers=0,\n",
    "    logging_dir=\".\\logs\",\n",
    "    logging_steps=100, \n",
    "    report_to=[\"tensorboard\"],  \n",
    "    eval_strategy=\"steps\",\n",
    "    fp16=True,                   \n",
    "    remove_unused_columns=False, \n",
    "    label_names= [\"labels\"]\n",
    ")\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        if \"num_items_in_batch\" in inputs:\n",
    "            inputs.pop(\"num_items_in_batch\")\n",
    "        return super().compute_loss(model, inputs, return_outputs=return_outputs)\n",
    "\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4921bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798407a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d758b22",
   "metadata": {},
   "source": [
    "### Fine-Tune BioMedClip + Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a98557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"YYYWei/path-va\")\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebe7c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c05a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "clip_model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(\n",
    "    'hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224',\n",
    "    device=device\n",
    ")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",           # Best quality for 4bit\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Use bfloat16 math for speed & stability\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "#model.config.use_cache = False  # Disable cache for LLM\n",
    "\n",
    "# LLM\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    use_fast=True\n",
    ")\n",
    "#llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "if llm_tokenizer.bos_token_id is None:\n",
    "    llm_tokenizer.add_special_tokens({'bos_token': '<bos>'})\n",
    "if llm_tokenizer.eos_token_id is None:\n",
    "    llm_tokenizer.add_special_tokens({'eos_token': '<eos>'})\n",
    "llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "\n",
    "# Dummy image (black image)\n",
    "dummy = preprocess_val(Image.new(\"RGB\", (224, 224))).unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    dummy_feat = clip_model.encode_image(dummy)  # shape: [1, D]\n",
    "\n",
    "vision_feature_dim = dummy_feat.shape[-1]  # This is what we need\n",
    "\n",
    "projection = nn.Linear(vision_feature_dim, llm_model.config.hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da804259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BiomedCLIPCaptionDataset(Dataset):\n",
    "    def __init__(self, items):\n",
    "        self.items = items\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.items[idx]\n",
    "        img = item[\"image\"].convert(\"RGB\")\n",
    "        caption = item[\"answer\"]\n",
    "        img_t = preprocess_val(img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            vis_emb = clip_model.encode_image(img_t)\n",
    "        vis_proj = projection(vis_emb).squeeze(0).cpu()\n",
    "        return {\"vision_prefix\": vis_proj, \"caption\": caption}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca18bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    vision_prefix = torch.stack([b[\"vision_prefix\"] for b in batch])\n",
    "    captions = [b[\"caption\"] for b in batch]\n",
    "\n",
    "    # Tokenize the fixed textual prompt 'Caption:'\n",
    "    prompt_texts = [\"Caption:\"] * len(batch)\n",
    "    prompt_tok = llm_tokenizer(\n",
    "        prompt_texts,\n",
    "        add_special_tokens=False,\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    prompt_ids = prompt_tok.input_ids  # [B, P]\n",
    "    prompt_mask = prompt_tok.attention_mask\n",
    "\n",
    "    # Tokenize the ground-truth captions\n",
    "    cap_tok = llm_tokenizer(\n",
    "        captions,\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    cap_ids = cap_tok.input_ids      # [B, C]\n",
    "    cap_mask = cap_tok.attention_mask\n",
    "\n",
    "    # Build input_ids: [bos slot] + prompt_ids + cap_ids\n",
    "    bos_id = llm_tokenizer.bos_token_id\n",
    "    B = len(batch)\n",
    "    bos_col = torch.full((B,1), bos_id, dtype=torch.long)\n",
    "    input_ids = torch.cat([bos_col.to(cap_ids.device), prompt_ids, cap_ids], dim=1)\n",
    "\n",
    "    # Labels: mask bos and prompt tokens, then captions\n",
    "    labels = torch.cat([\n",
    "        torch.full((B,1), -100, dtype=torch.long),\n",
    "        torch.full_like(prompt_ids, -100),\n",
    "        cap_ids.clone()\n",
    "    ], dim=1)\n",
    "\n",
    "    # Attention mask: 1 for bos slot + prompt + caption\n",
    "    attention_mask = torch.cat([\n",
    "        torch.ones((B,1), dtype=prompt_mask.dtype),\n",
    "        prompt_mask,\n",
    "        cap_mask\n",
    "    ], dim=1)\n",
    "\n",
    "    return {\n",
    "        \"vision_prefix\": vision_prefix,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd973d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    #task_type=\"CAUSAL_LM\",\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n",
    ")\n",
    "\n",
    "peft_llm = get_peft_model(llm_model, lora_config)\n",
    "peft_llm.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d83876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualInstructMistral(nn.Module):\n",
    "    def __init__(self, peft_llm):\n",
    "        super().__init__()\n",
    "        self.llm = peft_llm\n",
    "\n",
    "    def forward(self, input_ids=None, vision_prefix=None, attention_mask=None, labels=None):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        inputs_embeds = self.llm.get_input_embeddings()(input_ids)\n",
    "        inputs_embeds[:, 0, :] = vision_prefix.to(device)\n",
    "        return self.llm(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels.to(device) if labels is not None else None\n",
    "        )\n",
    "\n",
    "    def generate(self, input_ids, attention_mask, vision_prefix, **gen_kwargs):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        inputs_embeds = self.llm.get_input_embeddings()(input_ids)\n",
    "        inputs_embeds[:, 0, :] = vision_prefix.to(device)\n",
    "        return self.llm.generate(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            bos_token_id=llm_tokenizer.bos_token_id,\n",
    "            eos_token_id=llm_tokenizer.eos_token_id,\n",
    "            pad_token_id=llm_tokenizer.pad_token_id,\n",
    "            **gen_kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "model = VisualInstructMistral(peft_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75f6cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = random.sample(range(len(train_dataset)), 4000)\n",
    "val_indices   = random.sample(range(len(val_dataset)), 800)\n",
    "\n",
    "train_list = [train_dataset[i] for i in train_indices]\n",
    "val_list   = [val_dataset[i] for i in val_indices]\n",
    "\n",
    "train_ds = BiomedCLIPCaptionDataset(train_list)\n",
    "val_ds   = BiomedCLIPCaptionDataset(val_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325b7eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_steps=50,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=50, # logs loss every 100 steps\n",
    "    report_to=[\"tensorboard\"],  # log to TensorBoard\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    #save_steps=200,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    dataloader_num_workers=0,\n",
    "    label_names=[\"labels\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=llm_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd30ae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
